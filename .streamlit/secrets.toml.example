# .toml.example   (do NOT commit .toml)

# =========================
# OpenAI (default provider)
# =========================
OPENAI_API_KEY = "your-openai-api-key"
# Chat model options: gpt-5, gpt-4o, gpt-4o-mini (or any other OpenAi model you choose) 
OPENAI_CHAT_MODEL = "gpt-5-nano-2025-08-07"
# Embedding model options: text-embedding-3-small, text-embedding-3-large
OPENAI_EMBEDDING_MODEL = "text-embedding-3-small"

# =========================
# Supabase (server-side)
# =========================
SUPABASE_URL = "your-supabase-url"
SUPABASE_ANON_KEY = "yet-another-supabase-anon-key"
SUPABASE_SERVICE_KEY = "yet-another-supabase-service-key"

# ===========
# RAG tuning
# ===========
# If you reingest, you can experiment with 700–900 and overlap 120–150
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200
MAX_TOKENS_PER_CHUNK = 1500

# Retrieval (env-driven, no UI controls)
# Typical thresholds: 0.20–0.35. We recommend 0.25 to start.
SIMILARITY_THRESHOLD = 0.25
# How many chunks to keep in context
MAX_CONTEXT_CHUNKS = 10

# ========================
# Authentication mode
# ========================
# Choose one:
#  - azure   → App Service Authentication (Entra ID / "Easy Auth")
#  - public  → no login required (local dev, quick demos)
#  - custom  → for plugging in your own provider (e.g., Auth0/Supabase Auth)
AUTH_MODE = "public"         #Adjusted for Streamlit deployment

# ========================
# Logging / runtime
# ========================
LOG_LEVEL = "INFO"
PYTHONUNBUFFERED = 1
# ========================
